from pathlib import Path
from os.path import dirname

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

res_inter_dir = res_dir / "intermediate"
res_asm_dir = res_inter_dir / "asm" / "{hap}"
res_ref_dir = res_inter_dir / "ref" / "{ref}"
res_compbed_dir = res_inter_dir / "comparison" / "bed" / "{ref}"

final_dir = res_dir / "final" / "{ref}"

HAPLOTYPES = ["pat", "mat"]

ASM = {
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.mat.fasta.gz",
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.pat.fasta.gz",
}

REF = {
    "GRCh38": "https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz",
}

BENCH = {
    "GRCh38": {
        "vcf": "https://giab-data.s3.amazonaws.com/defrabb_runs/20240716_v0.018_HG002Q100/results/evaluations/happy/GRCh38_HG002_T~T2TQ100v1.1_Q~HPRCcur.20211005-DB-gtfix_TR~dipbed_GRCh38_HG002-T2TQ100v1.1-dipz2k_smvar-excluded/GRCh38_HG2-HPRCcur.20211005-DB-gtfix_HG2-T2TQ100-V1.1_smvar_dipcall-z2k.vcf.gz",
        "bed": "https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/analysis/NIST_HG002_DraftBenchmark_defrabbV0.018-20240716/GRCh38_HG2-T2TQ100-V1.1_smvar.benchmark.bed",
        "query_vcf": "https://giab-data.s3.amazonaws.com/giab-test-data/benchmarksets/HG002/draft-benchmarks/20220610_v0.008_HG002-HPRC-GRCh38/GRCh38_HG2-HPRC-20211005_dipcall-z2k_gtfix.vcf.gz",
    },
}

ERRORS = {
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchpatv1.1/hg002_curated_pat.errortype.hg002v1.1.bed",
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchmatv1.1/hg002_curated_mat.errortype.hg002v1.1.bed",
}

wildcard_constraints:
    mapped_hap="(mat|pat)"


################################################################################
# download a bunch of stuff


rule clone_liftover_scripts:
    output:
        directory(src_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.4.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


rule download_asm:
    output:
        src_dir / "asm" / "{hap}.fa.gz",
    params:
        url=lambda w: ASM[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


use rule download_asm as download_ref with:
    output:
        src_dir / "references" / "{ref}.fa.gz",
    params:
        url=lambda w: REF[w.ref],


use rule download_asm as download_bench_vcf with:
    output:
        src_dir / "references" / "q100_bench_{ref}.vcf.gz",
    params:
        url=lambda w: BENCH[w.ref]["vcf"],


use rule download_asm as download_query_vcf with:
    output:
        src_dir / "references" / "q100_query_{ref}.vcf.gz",
    params:
        url=lambda w: BENCH[w.ref]["query_vcf"],


use rule download_asm as download_bench_bed with:
    output:
        src_dir / "references" / "q100_bench_{ref}.bed",
    params:
        url=lambda w: BENCH[w.ref]["bed"],


rule download_errors:
    output:
        src_dir / "errors" / "{hap}.bed",
    params:
        url=lambda w: ERRORS[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


################################################################################
# make pafs


rule make_asm_genome:
    input:
        rules.download_asm.output,
    output:
        res_asm_dir / "genome.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1-2 > {output}
        """


use rule make_asm_genome as make_ref_genome with:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "genome.txt",
        

# for testing
rule list_test_asm_chrs:
    input:
        rules.make_asm_genome.output,
    output:
        res_asm_dir / "chrs_filtered.txt",
    shell:
        """
        cat {input} | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


rule filter_test_asm:
    input:
        fa=rules.download_asm.output,
        regions=rules.list_test_asm_chrs.output,
    output:
        res_asm_dir / "filtered.fa.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input.fa} \
        $(cat {input.regions} | tr '\n' ' ') | \
        bgzip -c \
        > {output}
        """


rule index_test_asm:
    input:
        rules.filter_test_asm.output,
    output:
        rules.filter_test_asm.output[0] + ".fai",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input}
        """


rule list_test_ref_chrs:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "chrs_filtered.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


use rule filter_test_asm as filter_test_ref with:
    input:
        fa=rules.download_ref.output,
        regions=rules.list_test_ref_chrs.output,
    output:
        res_ref_dir / "filtered.fa.gz",


use rule index_test_asm as index_test_ref with:
    input:
        rules.filter_test_ref.output,
    output:
        rules.filter_test_ref.output[0] + ".fai",


use rule index_test_asm as index_asm with:
    input:
        rules.download_asm.output,
    output:
        rules.download_asm.output[0] + ".fai",


use rule index_test_asm as index_ref with:
    input:
        rules.download_ref.output,
    output:
        rules.download_ref.output[0] + ".fai",


rule unzip_fasta:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "unzipped.fa"
    shell:
        "gunzip -c {input} > {output}"


rule run_minimap:
    input:
        # haplotype=rules.filter_test_asm.output,
        # _haplotype_idx=rules.index_test_asm.output,
        # ref=rules.filter_test_ref.output,
        # _ref_idx=rules.index_test_ref.output,
        haplotype=rules.download_asm.output,
        _haplotype_idx=rules.index_asm.output,
        ref=rules.download_ref.output,
        _ref_idx=rules.index_ref.output,
    output:
        res_compbed_dir / "{hap}.paf",
    conda:
        "envs/minimap.yml"
    log:
        res_compbed_dir / "{hap}.log",
    threads: 16
    resources:
        mem_mb=48000,
    shell:
        """
        minimap2 -c --paf-no-hit -t{threads} --cs -z200000,10000,200 -xasm5 \
          {input.ref} \
          {input.haplotype} \
          2> {log} > {output}
        """


################################################################################
# run liftover


rule slop_errors:
    input:
        rules.download_errors.output,
    output:
        res_asm_dir / "errors_slop50.bed",
    shell:
        """
        awk '{FS=OFS=\"\t\"} {print $1,$2-50,$3+50,$10,$11,$12}' {input} \
        > {output}
        """


rule split_errors:
    input:
        bed=rules.download_errors.output,
        genome=lambda w: expand(rules.make_asm_genome.output, hap = w.mapped_hap),
    output:
        res_asm_dir / "errors_slop50_{mapped_hap}.bed",
    params:
        chr_filter=lambda w: "PATERNAL" if w.mapped_hap == "pat" else "MATERNAL",
    conda:
        "envs/minimap.yml"
    shell:
        """
        grep {params.chr_filter} {input.bed} | \
        sed 's/\t/;/g4' | \
        bedtools sort -i stdin -g {input.genome} | \
        bedtools merge -i stdin -c 4 -o collapse -delim '~' \
        > {output} 
        """


rule run_liftover:
    input:
        paf=lambda w: expand(
            rules.run_minimap.output,
            allow_missing=True,
            hap=w.mapped_hap,
        ),
        bed=rules.split_errors.output,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}_projectable.bed",
        projected=res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}_projected.bed",
    threads: 8
    log:
        res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}.log",
    resources:
        mem_mb=32000,
    conda:
        "envs/flagger.yml"
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode asm2ref \
        --divergence \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads} \
        > {log}
        """


rule format_projected:
    input:
        bed=rules.run_liftover.output.projected,
        genome=rules.make_ref_genome.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected.bed.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/format.R"


rule nuc_projections:
    input:
        bed=rules.format_projected.output,
        fa=rules.unzip_fasta.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_nuc.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        gunzip -c {input.bed} | \
        cut -f 1,2,3 | \
        nucBed -bed stdin -fi {input.fa} -seq | \
        cut -f13 | \
        tail -n+2 | \
        paste <(gunzip -c {input.bed}) - | \
        gzip -c > {output}
        """


################################################################################
# do fun stats


rule get_small_variants:
    input:
        vcf=rules.download_bench_vcf.output,
        bed=rules.download_bench_bed.output,
    output:
        res_ref_dir / "small_vars.vcf.gz"
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vcf} -b {input.bed} -header | \
        bgzip -c > {output}
        """


use rule get_small_variants as get_small_dipcall_variants with:
    input:
        vcf=rules.download_query_vcf.output,
        bed=rules.download_bench_bed.output,
    output:
        res_ref_dir / "small_dipcall_vars.vcf.gz"


# remove the AD field from the dipcall vcf so that the merge doesn't clobber
# the format field below
rule strip_gt_dipcall_variants:
    input:
        rules.get_small_dipcall_variants.output,
    output:
        res_ref_dir / "small_dipcall_vars_gtonly.vcf.gz"
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools annotate -x 'FORMAT/AD' {input} | bgzip -c > {output}
        """


rule index_small_variants:
    input:
        rules.get_small_variants.output,
    output:
        rules.get_small_variants.output[0] + ".tbi"
    conda:
        "envs/minimap.yml"
    shell:
        """
        tabix {input}
        """


use rule index_small_variants as index_small_dipcall_variants with:
    input:
        rules.strip_gt_dipcall_variants.output,
    output:
        rules.strip_gt_dipcall_variants.output[0] + ".tbi"


rule merge_happy_dipcall:
    input:
        _happy_idx=rules.index_small_variants.output,
        _dipcall_idx=rules.index_small_dipcall_variants.output,
        happy=rules.get_small_variants.output,
        dipcall=rules.strip_gt_dipcall_variants.output
    output:
        res_ref_dir / "small_merged.vcf.gz"
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools merge {input.happy} {input.dipcall} | \
        awk 'OFS="\t" {{ if($9!="GT") {{print $0}}}}' | \
        bgzip -c > {output}
        """


rule phase_happy:
    input:
        happy=rules.get_small_variants.output,
        merged=rules.merge_happy_dipcall.output,
    output:
        fixed=res_ref_dir / "happy_phased_fixed.vcf.gz",
        notfixed=res_ref_dir / "happy_phased_notfixed.vcf.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/phase_happy.R"


rule vcf_to_bed:
    input:
        rules.phase_happy.output.fixed,
    output:
        res_ref_dir / "small_vars_{mapped_hap}.bed.gz"
    script:
        "scripts/python/vcf_to_bed.py"


rule add_expectation_vbench:
    input:
        rules.vcf_to_bed.output,
    output:
        res_ref_dir / "expected" / "small_vars_{mapped_hap}_expected.bed.gz"
    conda:
        "envs/r.yml"
    script:
        "scripts/R/add_expectation.R"


#rule filter_fn_fp:
#    input:
#        rules.vcf_to_bed.output,
#    output:
#        res_ref_dir / "small_vars_{mapped_hap}_fp_fn.bed.gz"
#    shell:
#        "gunzip -c {input} | grep -E '	(FP|FN)' | gzip -c > {output}"
#
#
#rule filter_tp:
#    input:
#        rules.vcf_to_bed.output,
#    output:
#        res_ref_dir / "small_vars_{mapped_hap}_tp.bed.gz"
#    shell:
#        "gunzip -c {input} | grep -v -E '	(FP|FN)' | gzip -c > {output}"


rule small_var_projected:
    input:
        bed=rules.nuc_projections.output,
        regions=rules.download_bench_bed.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_smallvar.bed.gz",
    conda:
        "envs/minimap.yml"
    # NOTE this can produce duplicates if the bed in 'a' intersects with more
    # than one region in 'b' so use funny awk thing to remove these
    #
    # NOTE2 the funny sed thing is to flip the order of columns 2-3 with 4-5 (and back)
    # columns 2-3 have the slopped coordinates and 4-5 have the original coordinates,
    # which we want to use when intersecting. The sed command is also delightfully
    # absurd to look at :)
    shell:
        """
        gunzip -c {input.bed} | \
        sed 's/^\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\(.*\)/\\1\t\\4\t\\5\t\\2\t\\3\t\\6/' | \
        intersectBed -a stdin -b {input.regions} -wa | \
        awk '!a[$0]++' | \
        sed 's/^\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\(.*\)/\\1\t\\4\t\\5\t\\2\t\\3\t\\6/' | \
        gzip -c > {output}
        """


rule get_vbench_overlaps:
    input:
        #vbench=rules.filter_fn_fp.output,
        vbench=rules.add_expectation_vbench.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_vbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vbench} -b {input.gbench} -loj | gzip -c > {output}
        """


rule get_gbench_overlaps:
    input:
        #vbench=rules.filter_fn_fp.output,
        vbench=rules.add_expectation_vbench.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_gbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -b {input.vbench} -a {input.gbench} -loj | gzip -c > {output}
        """


rule summarize_bench:
    input:
        gbench=expand(
            rules.get_gbench_overlaps.output,
            #+ rules.get_variants_in_gbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
        vbench=expand(
            rules.get_vbench_overlaps.output,
            #+ rules.get_variants_in_vbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
    output:
        vbench=final_dir / "variant_summary" / "vbench.tsv.gz",
        gbench=final_dir / "variant_summary" / "gbench.tsv.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/summarize.R"


rule all:
    input:
        expand(
            rules.summarize_bench.output,
            ref=["GRCh38"],
        ),
        #expand(
        #    rules.filter_tp.output,
        #    ref=["GRCh38"],
        #    mapped_hap=HAPLOTYPES,
        #),
