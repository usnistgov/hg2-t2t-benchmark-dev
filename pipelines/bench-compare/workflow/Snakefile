from pathlib import Path
from os.path import dirname

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

res_inter_dir = res_dir / "intermediate"
res_asm_dir = res_inter_dir / "asm" / "{hap}"
res_ref_dir = res_inter_dir / "ref" / "{ref}"
res_compbed_dir = res_inter_dir / "comparison" / "bed" / "{ref}"

final_dir = res_dir / "final" / "{ref}"

HAPLOTYPES = ["pat", "mat"]

ASM = {
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.mat.fasta.gz",
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.pat.fasta.gz",
}

REF = {
    "GRCh38": "https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz",
}

BENCH = {
    "GRCh38": {
        "vcf": "https://giab-data.s3.amazonaws.com/defrabb_runs/20240716_v0.018_HG002Q100/results/evaluations/happy/GRCh38_HG002_T~T2TQ100v1.1_Q~HPRCcur.20211005-DB-gtfix_TR~dipbed_GRCh38_HG002-T2TQ100v1.1-dipz2k_smvar-excluded/GRCh38_HG2-HPRCcur.20211005-DB-gtfix_HG2-T2TQ100-V1.1_smvar_dipcall-z2k.vcf.gz",
        "bed": "https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/analysis/NIST_HG002_DraftBenchmark_defrabbV0.018-20240716/GRCh38_HG2-T2TQ100-V1.1_smvar.benchmark.bed",
    },
}

ERRORS = {
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchpatv1.1/hg002_curated_pat.errortype.hg002v1.1.bed",
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchmatv1.1/hg002_curated_mat.errortype.hg002v1.1.bed",
}

wildcard_constraints:
    mapped_hap="(mat|pat)"


################################################################################
# download a bunch of stuff


rule clone_liftover_scripts:
    output:
        directory(src_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.4.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


rule download_asm:
    output:
        src_dir / "asm" / "{hap}.fa.gz",
    params:
        url=lambda w: ASM[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


use rule download_asm as download_ref with:
    output:
        src_dir / "references" / "{ref}.fa.gz",
    params:
        url=lambda w: REF[w.ref],


use rule download_asm as download_bench_vcf with:
    output:
        src_dir / "references" / "q100_bench_{ref}.vcf.gz",
    params:
        url=lambda w: BENCH[w.ref]["vcf"],


use rule download_asm as download_bench_bed with:
    output:
        src_dir / "references" / "q100_bench_{ref}.bed",
    params:
        url=lambda w: BENCH[w.ref]["bed"],


rule download_errors:
    output:
        src_dir / "errors" / "{hap}.bed",
    params:
        url=lambda w: ERRORS[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


################################################################################
# make pafs


rule make_asm_genome:
    input:
        rules.download_asm.output,
    output:
        res_asm_dir / "genome.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1-2 > {output}
        """


use rule make_asm_genome as make_ref_genome with:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "genome.txt",
        

# for testing
rule list_test_asm_chrs:
    input:
        rules.make_asm_genome.output,
    output:
        res_asm_dir / "chrs_filtered.txt",
    shell:
        """
        cat {input} | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


rule filter_test_asm:
    input:
        fa=rules.download_asm.output,
        regions=rules.list_test_asm_chrs.output,
    output:
        res_asm_dir / "filtered.fa.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input.fa} \
        $(cat {input.regions} | tr '\n' ' ') | \
        bgzip -c \
        > {output}
        """


rule index_test_asm:
    input:
        rules.filter_test_asm.output,
    output:
        rules.filter_test_asm.output[0] + ".fai",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input}
        """


rule list_test_ref_chrs:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "chrs_filtered.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


use rule filter_test_asm as filter_test_ref with:
    input:
        fa=rules.download_ref.output,
        regions=rules.list_test_ref_chrs.output,
    output:
        res_ref_dir / "filtered.fa.gz",


use rule index_test_asm as index_test_ref with:
    input:
        rules.filter_test_ref.output,
    output:
        rules.filter_test_ref.output[0] + ".fai",


use rule index_test_asm as index_asm with:
    input:
        rules.download_asm.output,
    output:
        rules.download_asm.output[0] + ".fai",


use rule index_test_asm as index_ref with:
    input:
        rules.download_ref.output,
    output:
        rules.download_ref.output[0] + ".fai",


rule unzip_fasta:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "unzipped.fa"
    shell:
        "gunzip -c {input} > {output}"


rule run_minimap:
    input:
        # haplotype=rules.filter_test_asm.output,
        # _haplotype_idx=rules.index_test_asm.output,
        # ref=rules.filter_test_ref.output,
        # _ref_idx=rules.index_test_ref.output,
        haplotype=rules.download_asm.output,
        _haplotype_idx=rules.index_asm.output,
        ref=rules.download_ref.output,
        _ref_idx=rules.index_ref.output,
    output:
        res_compbed_dir / "{hap}.paf",
    conda:
        "envs/minimap.yml"
    log:
        res_compbed_dir / "{hap}.log",
    threads: 16
    resources:
        mem_mb=48000,
    shell:
        """
        minimap2 -c --paf-no-hit -t{threads} --cs -z200000,10000,200 -xasm5 \
          {input.ref} \
          {input.haplotype} \
          2> {log} > {output}
        """


################################################################################
# run liftover


rule slop_errors:
    input:
        rules.download_errors.output,
    output:
        res_asm_dir / "errors_slop50.bed",
    shell:
        """
        awk '{FS=OFS=\"\t\"} {print $1,$2-50,$3+50,$10,$11,$12}' {input} \
        > {output}
        """


rule split_errors:
    input:
        bed=rules.download_errors.output,
        genome=lambda w: expand(rules.make_asm_genome.output, hap = w.mapped_hap),
    output:
        res_asm_dir / "errors_slop50_{mapped_hap}.bed",
    params:
        chr_filter=lambda w: "PATERNAL" if w.mapped_hap == "pat" else "MATERNAL",
    conda:
        "envs/minimap.yml"
    shell:
        """
        grep {params.chr_filter} {input.bed} | \
        sed 's/\t/;/g4' | \
        bedtools sort -i stdin -g {input.genome} | \
        bedtools merge -i stdin -c 4 -o collapse -delim '~' \
        > {output} 
        """


rule run_liftover:
    input:
        paf=lambda w: expand(
            rules.run_minimap.output,
            allow_missing=True,
            hap=w.mapped_hap,
        ),
        bed=rules.split_errors.output,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}_projectable.bed",
        projected=res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}_projected.bed",
    threads: 8
    log:
        res_compbed_dir / "hprc_{hap}_q100_{mapped_hap}.log",
    resources:
        mem_mb=32000,
    conda:
        "envs/flagger.yml"
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode asm2ref \
        --divergence \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads} \
        > {log}
        """


rule format_projected:
    input:
        bed=rules.run_liftover.output.projected,
        genome=rules.make_ref_genome.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected.bed.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/format.R"


rule nuc_projections:
    input:
        bed=rules.format_projected.output,
        fa=rules.unzip_fasta.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_nuc.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        gunzip -c {input.bed} | \
        cut -f 1,2,3 | \
        nucBed -bed stdin -fi {input.fa} -seq | \
        cut -f13 | \
        tail -n+2 | \
        paste <(gunzip -c {input.bed}) - | \
        gzip -c > {output}
        """


################################################################################
# do fun stats


rule get_small_variants:
    input:
        vcf=rules.download_bench_vcf.output,
        bed=rules.download_bench_bed.output,
    output:
        res_ref_dir / "small_vars.vcf.gz"
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vcf} -b {input.bed} | \
        gzip -c > {output}
        """


rule vcf_to_bed:
    input:
        rules.get_small_variants.output,
    output:
        res_ref_dir / "small_vars_{mapped_hap}.bed.gz"
    script:
        "scripts/python/vcf_to_bed.py"


rule filter_fn_fp:
    input:
        rules.vcf_to_bed.output,
    output:
        res_ref_dir / "small_vars_{mapped_hap}_fp_fn.bed.gz"
    shell:
        "gunzip -c {input} | grep -E '	(FP|FN)' | gzip -c > {output}"


rule filter_tp:
    input:
        rules.vcf_to_bed.output,
    output:
        res_ref_dir / "small_vars_{mapped_hap}_tp.bed.gz"
    shell:
        "gunzip -c {input} | grep -v -E '	(FP|FN)' | gzip -c > {output}"


rule small_var_projected:
    input:
        bed=rules.nuc_projections.output,
        regions=rules.download_bench_bed.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_smallvar.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.bed} -b {input.regions} -wa | gzip -c > {output}
        """


rule get_vbench_overlaps:
    input:
        vbench=rules.filter_fn_fp.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_vbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vbench} -b {input.gbench} -loj | gzip -c > {output}
        """


rule get_gbench_overlaps:
    input:
        vbench=rules.filter_fn_fp.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_gbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -b {input.vbench} -a {input.gbench} -loj | gzip -c > {output}
        """


#rule get_variants_in_vbench:
#    input:
#        vcf=rules.filter_variants.output,
#        bed=rules.small_var_projected.output,
#    output:
#        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "{label}_vbench.vcf.gz",
#    conda:
#        "envs/minimap.yml"
#    shell:
#        """
#        subtractBed -a {input.vcf} -b {input.bed} -A | gzip -c > {output}
#        """
#
#
#rule get_variants_in_gbench:
#    input:
#        vcf=rules.filter_variants.output,
#        bed=rules.small_var_projected.output,
#    output:
#        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "{label}_gbench.bed",
#    conda:
#        "envs/minimap.yml"
#    shell:
#        """
#        subtractBed -b {input.vcf} -a {input.bed} -A > {output}
#        """


rule summarize_bench:
    input:
        gbench=expand(
            rules.get_gbench_overlaps.output,
            #+ rules.get_variants_in_gbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
        vbench=expand(
            rules.get_vbench_overlaps.output,
            #+ rules.get_variants_in_vbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
    output:
        vbench=final_dir / "variant_summary" / "vbench.tsv.gz",
        gbench=final_dir / "variant_summary" / "gbench.tsv.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/summarize.R"


rule all:
    input:
        expand(
            rules.summarize_bench.output,
            ref=["GRCh38"],
        ),
        expand(
            rules.filter_tp.output,
            ref=["GRCh38"],
            mapped_hap=HAPLOTYPES,
        ),
