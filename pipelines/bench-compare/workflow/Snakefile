from pathlib import Path
from os.path import dirname

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

res_inter_dir = res_dir / "intermediate"
res_asm_dir = res_inter_dir / "asm" / "{hap}"
res_ref_dir = res_inter_dir / "ref" / "{ref}"
res_compbed_dir = res_inter_dir / "comparison" / "bed" / "{ref}"

final_dir = res_dir / "final" / "{ref}" / "{qver}"

HAPLOTYPES = ["pat", "mat"]

ASM = {
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.mat.fasta.gz",
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v1.1.pat.fasta.gz",
}

REF = {
    "GRCh38": "https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz",
}

BENCH = {
    "GRCh38": {
        "vcf": "https://giab-data.s3.amazonaws.com/defrabb_runs/20241009_v0.018_HG002Q100v1.1/results/evaluations/happy/GRCh38_HG002_T~T2TQ100v1.1_Q~HPRCcur.20211005-DB-gtfix_TR~dipbed_GRCh38_HG002-T2TQ100v1.1-dipz2k_smvar-excluded/GRCh38_HG2-HPRCcur.20211005-DB-gtfix_HG2-T2TQ100-V1.1_smvar_dipcall-z2k.vcf.gz",
        "bed": "https://giab-data.s3.amazonaws.com/defrabb_runs/20241009_v0.018_HG002Q100v1.1/results/draft_benchmarksets/GRCh38_HG002-T2TQ100v1.1-dipz2k_smvar-excluded/GRCh38_HG2-T2TQ100-V1.1_smvar_dipcall-z2k.benchmark.bed",
        "query_vcf": "https://giab-data.s3.amazonaws.com/giab-test-data/benchmarksets/HG002/draft-benchmarks/20220610_v0.008_HG002-HPRC-GRCh38/GRCh38_HG2-HPRC-20211005_dipcall-z2k_gtfix.vcf.gz",
    },
}

ERRORS = {
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchpatv1.1/hg002_curated_pat.errortype.hg002v1.1.bed",
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/benchmatv1.1/hg002_curated_mat.errortype.hg002v1.1.bed",
}

HPRC_BAM = {
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/hg002_curated_pat_vs_diploid.sort.bam",
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/polishing/HG002/v1.1/benchmark/results/hprc_hg002_curated_vs_v1.1/hg002_curated_mat_vs_diploid.sort.bam",
}

ERROR_TARBALLS = {
    "pat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/benchmarking/q100_output_tarballs/hprc_year1_curated/hprc_curated_pat.qualiffyoutput.tar.gz",
    "mat": "https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/benchmarking/q100_output_tarballs/hprc_year1_curated/hprc_curated_mat.qualiffyoutput.tar.gz",
}

wildcard_constraints:
    mapped_hap="(mat|pat)"


################################################################################
# download a bunch of stuff


rule clone_liftover_scripts:
    output:
        directory(src_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.4.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


rule download_asm:
    output:
        src_dir / "asm" / "{hap}.fa.gz",
    params:
        url=lambda w: ASM[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


use rule download_asm as download_ref with:
    output:
        src_dir / "references" / "{ref}.fa.gz",
    params:
        url=lambda w: REF[w.ref],


use rule download_asm as download_bench_vcf with:
    output:
        src_dir / "references" / "q100_bench_{ref}.vcf.gz",
    params:
        url=lambda w: BENCH[w.ref]["vcf"],


use rule download_asm as download_query_vcf with:
    output:
        src_dir / "references" / "q100_query_{ref}.vcf.gz",
    params:
        url=lambda w: BENCH[w.ref]["query_vcf"],


use rule download_asm as download_bench_bed with:
    output:
        src_dir / "references" / "q100_bench_{ref}.bed",
    params:
        url=lambda w: BENCH[w.ref]["bed"],


use rule download_asm as download_error_tarball with:
    output:
        src_dir / "errors" / "v2" / "{hap}.tar.gz",
    params:
        url=lambda w: ERROR_TARBALLS[w.hap],


rule extract_errors:
    input:
        rules.download_error_tarball.output,
    output:
        src_dir / "errors" / "v2" / "{hap}.bed",
    shell:
        """
        tar xzfO {input} hprc_curated_{wildcards.hap}/hprc_curated_{wildcards.hap}.errortype.v1.1.bed > {output}
        """


rule extract_bam:
    input:
        rules.download_error_tarball.output,
    output:
        src_dir / "errors" / "v2" / "{hap}.bam",
    shell:
        """
        tar xzfO {input} hprc_curated_{wildcards.hap}/hprc_curated_{wildcards.hap}_vs_v1.1.trimmedphased.split.sort.bam > {output}
        """


rule download_errors:
    output:
        src_dir / "errors" / "v1" / "{hap}.bed",
    params:
        url=lambda w: ERRORS[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


rule download_error_bam:
    output:
        src_dir / "errors" / "v1" / "{hap}.bam",
    params:
        url=lambda w: HPRC_BAM[w.hap],
    shell:
        "curl -SsLqf {params.url} > {output}"


################################################################################
# make pafs


rule make_asm_genome:
    input:
        rules.download_asm.output,
    output:
        res_asm_dir / "genome.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1-2 > {output}
        """


use rule make_asm_genome as make_ref_genome with:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "genome.txt",
        

# for testing
rule list_test_asm_chrs:
    input:
        rules.make_asm_genome.output,
    output:
        res_asm_dir / "chrs_filtered.txt",
    shell:
        """
        cat {input} | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


rule filter_test_asm:
    input:
        fa=rules.download_asm.output,
        regions=rules.list_test_asm_chrs.output,
    output:
        res_asm_dir / "filtered.fa.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input.fa} \
        $(cat {input.regions} | tr '\n' ' ') | \
        bgzip -c \
        > {output}
        """


rule index_test_asm:
    input:
        rules.filter_test_asm.output,
    output:
        rules.filter_test_asm.output[0] + ".fai",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input}
        """


rule list_test_ref_chrs:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "chrs_filtered.txt",
    conda:
        "envs/minimap.yml"
    shell:
        """
        samtools faidx {input} -o - | \
        cut -f1 | \
        grep -E '(chr21|chr22)' > {output}
        """


use rule filter_test_asm as filter_test_ref with:
    input:
        fa=rules.download_ref.output,
        regions=rules.list_test_ref_chrs.output,
    output:
        res_ref_dir / "filtered.fa.gz",


use rule index_test_asm as index_test_ref with:
    input:
        rules.filter_test_ref.output,
    output:
        rules.filter_test_ref.output[0] + ".fai",


use rule index_test_asm as index_asm with:
    input:
        rules.download_asm.output,
    output:
        rules.download_asm.output[0] + ".fai",


use rule index_test_asm as index_ref with:
    input:
        rules.download_ref.output,
    output:
        rules.download_ref.output[0] + ".fai",


rule unzip_fasta:
    input:
        rules.download_ref.output,
    output:
        res_ref_dir / "unzipped.fa"
    shell:
        "gunzip -c {input} > {output}"


rule run_minimap:
    input:
        # haplotype=rules.filter_test_asm.output,
        # _haplotype_idx=rules.index_test_asm.output,
        # ref=rules.filter_test_ref.output,
        # _ref_idx=rules.index_test_ref.output,
        haplotype=rules.download_asm.output,
        _haplotype_idx=rules.index_asm.output,
        ref=rules.download_ref.output,
        _ref_idx=rules.index_ref.output,
    output:
        res_compbed_dir / "{hap}.paf",
    conda:
        "envs/minimap.yml"
    log:
        res_compbed_dir / "{hap}.log",
    threads: 16
    resources:
        mem_mb=48000,
    shell:
        """
        minimap2 -c --paf-no-hit -t{threads} --cs -z200000,10000,200 -xasm5 \
          {input.ref} \
          {input.haplotype} \
          2> {log} > {output}
        """


################################################################################
# run liftover


rule fix_errors:
    input:
        lambda w: rules.download_errors.output if w["qver"] == "v1" else rules.extract_errors.output,
    output:
        res_asm_dir / "{qver}" / "errors_fixed.bed",
    script:
        "scripts/python/fix_errors.py"


rule get_error_coverage:
    input:
        bam=lambda w: rules.download_error_bam.output if w["qver"] == "v1" else rules.extract_bam.output,
        bed=rules.fix_errors.output,
    output:
        res_asm_dir / "{qver}" / "no_coverage.txt",
    conda:
        "envs/minimap.yml"
    # Get a list of all errors that are completely within a region of no
    # coverage. Low map alignments are not considered to "cover" asm.
    # Only regions 100+bp long are considered.
    # Return a list of "gid"s as newline separated list.
    shell:
        """
        samtools view -h -q 5 {input.bam} | \
        bedtools genomecov -ibam stdin -bga | \
        grep '0$' | \
        cut -f1-3 | \
        awk 'OFS="\t" {{ if ($3-$2>99) {{ print $0 }} }}' | \
        intersectBed -a {input.bed} -b stdin -f 1 -wa -u | \
        cut -f10 > {output}
        """


rule group_errors:
    input:
        bed=rules.fix_errors.output,
        nocov=rules.get_error_coverage.output,
    output:
        res_asm_dir / "{qver}" / "errors_grouped.bed",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/group_errors.R"


#rule slop_errors:
#    input:
#        rules.download_errors.output,
#    output:
#        res_asm_dir / "errors_slop50.bed",
#    shell:
#        """
#        awk '{FS=OFS=\"\t\"} {print $1,$2-50,$3+50,$10,$11,$12}' {input} \
#        > {output}
#        """


rule split_errors:
    input:
        bed=rules.group_errors.output,
        genome=lambda w: expand(rules.make_asm_genome.output, hap = w.mapped_hap),
    output:
        res_asm_dir / "{qver}" / "errors_slop50_{mapped_hap}.bed",
    params:
        chr_filter=lambda w: "PATERNAL" if w.mapped_hap == "pat" else "MATERNAL",
    conda:
        "envs/minimap.yml"
    # 1. filter errors to the hap we want
    # 2. join all columns after 3 using ";" so that we only have 4 columns
    #    total (required for projection script)
    # 3. sort bed
    # 4. merge any overlapping regions and join column 4 with "~"; this is
    #    because the projection script will get very angry if there are
    #    overlapping regions, and we will split the "~" columns after.
    shell:
        """
        grep {params.chr_filter} {input.bed} | \
        sed 's/\t/;/g4' | \
        bedtools sort -i stdin -g {input.genome} | \
        bedtools merge -i stdin -c 4 -o collapse -delim '~' \
        > {output} 
        """


rule run_liftover:
    input:
        paf=lambda w: expand(
            rules.run_minimap.output,
            allow_missing=True,
            hap=w.mapped_hap,
        ),
        bed=rules.split_errors.output,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=res_compbed_dir / "{qver}" / "hprc_{hap}_q100_{mapped_hap}_projectable.bed",
        projected=res_compbed_dir / "{qver}" / "hprc_{hap}_q100_{mapped_hap}_projected.bed",
    threads: 8
    log:
        res_compbed_dir / "hprc_{qver}_{hap}_q100_{mapped_hap}.log",
    resources:
        mem_mb=32000,
    conda:
        "envs/flagger.yml"
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode asm2ref \
        --divergence \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads} \
        > {log}
        """


rule format_projected:
    input:
        bed=rules.run_liftover.output.projected,
        genome=rules.make_ref_genome.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected.bed.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/format.R"


rule nuc_projections:
    input:
        bed=rules.format_projected.output,
        fa=rules.unzip_fasta.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_nuc.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        gunzip -c {input.bed} | \
        cut -f 1,2,3 | \
        nucBed -bed stdin -fi {input.fa} -seq | \
        cut -f13 | \
        tail -n+2 | \
        paste <(gunzip -c {input.bed}) - | \
        gzip -c > {output}
        """


################################################################################
# do fun stats


rule get_small_variants:
    input:
        vcf=rules.download_bench_vcf.output,
        bed=rules.download_bench_bed.output,
    output:
        res_ref_dir / "small_vars.vcf.gz"
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vcf} -b {input.bed} -header | \
        bgzip -c > {output}
        """


use rule get_small_variants as get_small_dipcall_variants with:
    input:
        vcf=rules.download_query_vcf.output,
        bed=rules.download_bench_bed.output,
    output:
        res_ref_dir / "small_dipcall_vars.vcf.gz"


# remove the AD field from the dipcall vcf so that the merge doesn't clobber
# the format field below
rule strip_gt_dipcall_variants:
    input:
        rules.get_small_dipcall_variants.output,
    output:
        res_ref_dir / "small_dipcall_vars_gtonly.vcf.gz"
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools annotate -x 'FORMAT/AD' {input} | bgzip -c > {output}
        """


rule index_small_variants:
    input:
        rules.get_small_variants.output,
    output:
        rules.get_small_variants.output[0] + ".tbi"
    conda:
        "envs/minimap.yml"
    shell:
        """
        tabix {input}
        """


use rule index_small_variants as index_small_dipcall_variants with:
    input:
        rules.strip_gt_dipcall_variants.output,
    output:
        rules.strip_gt_dipcall_variants.output[0] + ".tbi"


rule merge_happy_dipcall:
    input:
        _happy_idx=rules.index_small_variants.output,
        _dipcall_idx=rules.index_small_dipcall_variants.output,
        happy=rules.get_small_variants.output,
        dipcall=rules.strip_gt_dipcall_variants.output
    output:
        res_ref_dir / "small_merged.vcf.gz"
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools merge {input.happy} {input.dipcall} | \
        awk 'OFS="\t" {{ if($9!="GT") {{print $0}}}}' | \
        bgzip -c > {output}
        """


rule phase_happy:
    input:
        happy=rules.get_small_variants.output,
        merged=rules.merge_happy_dipcall.output,
    output:
        fixed=res_ref_dir / "happy_phased_fixed.vcf.gz",
        notfixed=res_ref_dir / "happy_phased_notfixed.vcf.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/phase_happy.R"


rule vcf_to_bed:
    input:
        rules.phase_happy.output.fixed,
    output:
        res_ref_dir / "small_vars_{mapped_hap}.bed.gz"
    script:
        "scripts/python/vcf_to_bed.py"


rule add_expectation_vbench:
    input:
        rules.vcf_to_bed.output,
    output:
        res_ref_dir / "expected" / "small_vars_{mapped_hap}_expected.bed.gz"
    conda:
        "envs/r.yml"
    script:
        "scripts/R/add_expectation.R"


#rule filter_fn_fp:
#    input:
#        rules.vcf_to_bed.output,
#    output:
#        res_ref_dir / "small_vars_{mapped_hap}_fp_fn.bed.gz"
#    shell:
#        "gunzip -c {input} | grep -E '	(FP|FN)' | gzip -c > {output}"
#
#
#rule filter_tp:
#    input:
#        rules.vcf_to_bed.output,
#    output:
#        res_ref_dir / "small_vars_{mapped_hap}_tp.bed.gz"
#    shell:
#        "gunzip -c {input} | grep -v -E '	(FP|FN)' | gzip -c > {output}"


rule small_var_projected:
    input:
        bed=rules.nuc_projections.output,
        regions=rules.download_bench_bed.output,
    output:
        final_dir / "projected" / "hprc_{hap}_q100_{mapped_hap}_projected_smallvar.bed.gz",
    conda:
        "envs/minimap.yml"
    # NOTE the funny sed thing is to flip the order of columns 2-3 with 4-5 (and back)
    # columns 2-3 have the slopped coordinates and 4-5 have the original coordinates,
    # which we want to use when intersecting. The sed command is also delightfully
    # absurd to look at :)
    shell:
        """
        gunzip -c {input.bed} | \
        sed 's/^\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\(.*\)/\\1\t\\4\t\\5\t\\2\t\\3\t\\6/' | \
        intersectBed -a stdin -b {input.regions} -wa -u | \
        sed 's/^\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\([^\t]\+\)\t\(.*\)/\\1\t\\4\t\\5\t\\2\t\\3\t\\6/' | \
        gzip -c > {output}
        """


rule get_projection_stats:
    input:
        projectable=rules.run_liftover.output.projectable,
        src=rules.split_errors.output,
        smallvar=rules.small_var_projected.output,
    output:
        counts_plot=final_dir / "projected" / "stats" / "hprc_{hap}_q100_{mapped_hap}_chrom_counts.pdf",
        groups_plot=final_dir / "projected" / "stats" / "hprc_{hap}_q100_{mapped_hap}_chrom_groups.pdf",
        groups_bed=final_dir / "projected" / "stats" / "hprc_{hap}_q100_{mapped_hap}_chrom_groups.bed.gz",
        smallvar_plot=final_dir / "projected" / "stats" / "hprc_{hap}_q100_{mapped_hap}_smallvar_counts.pdf",
        smallvar_overlaps_plot=final_dir / "projected" / "stats" / "hprc_{hap}_q100_{mapped_hap}_smallvar_overlaps.pdf",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/projection_stats.R"


rule get_vbench_overlaps:
    input:
        #vbench=rules.filter_fn_fp.output,
        vbench=rules.add_expectation_vbench.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_vbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -a {input.vbench} -b {input.gbench} -loj | gzip -c > {output}
        """


rule get_gbench_overlaps:
    input:
        #vbench=rules.filter_fn_fp.output,
        vbench=rules.add_expectation_vbench.output,
        gbench=rules.small_var_projected.output,
    output:
        final_dir / "variant_bench_compare" / "hprc_{hap}_q100_{mapped_hap}" / "both_gbench.bed.gz",
    conda:
        "envs/minimap.yml"
    shell:
        """
        intersectBed -b {input.vbench} -a {input.gbench} -loj | gzip -c > {output}
        """


rule summarize_bench:
    input:
        gbench=expand(
            rules.get_gbench_overlaps.output,
            #+ rules.get_variants_in_gbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
        vbench=expand(
            rules.get_vbench_overlaps.output,
            #+ rules.get_variants_in_vbench.output,
            allow_missing=True,
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            #label=["FP", "FN"],
        ),
    output:
        vbench=final_dir / "variant_summary" / "vbench.tsv.gz",
        gbench=final_dir / "variant_summary" / "gbench.tsv.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/summarize.R"


rule get_matches:
    input:
        vbench=rules.summarize_bench.output.vbench,
        gbench=rules.summarize_bench.output.gbench,
    output:
        hits = final_dir / "variant_summary" / "hits.tsv.gz",
        v_nohit = final_dir / "variant_summary" / "vbench_nohits.tsv.gz",
        g_nohit = final_dir / "variant_summary" / "gbench_nohits.tsv.gz",
        badlift = final_dir / "variant_summary" / "vbench_badlift.tsv.gz",
        duplicated = final_dir / "variant_summary" / "vbench_duplicated.tsv.gz",
        unfixable_overlap = final_dir / "variant_summary" / "unfixable_overlap.tsv.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/get_matches.R"


rule annotate_vcf:
    input:
        vcf=rules.phase_happy.output.fixed,
        hit=rules.get_matches.output.hits,
        v_nohit=rules.get_matches.output.v_nohit,
        g_nohit=rules.get_matches.output.g_nohit,
        gbench=rules.summarize_bench.output.gbench,
    output:
        all=final_dir / "match_summary" / "annotated_all.tsv.gz",
        hit=final_dir / "match_summary" / "annotated_hits.tsv.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/annotate_vcf.R"


rule cat_projectable:
    input:
        expand(
            rules.run_liftover.output.projectable,
            zip,
            allow_missing = True,
            hap = ["pat", "mat"],
            mapped_hap = ["pat", "mat"],
        ),
    output:
        res_ref_dir / "{qver}" / "projectable_all_haps.bed"
    shell:
        """
        cat {input} > {output}
        """


rule cat_split_errors:
    input:
        expand(
            rules.split_errors.output,
            zip,
            allow_missing = True,
            hap = ["pat", "mat"],
            mapped_hap = ["pat", "mat"],
        ),
    output:
        res_ref_dir / "{qver}" / "split_errors_all_haps.bed"
    shell:
        """
        cat {input} > {output}
        """


rule plot_match_summary:
    input:
        projectable=rules.cat_projectable.output,
        src=rules.cat_split_errors.output,
        smallvar_pat = expand(
            rules.small_var_projected.output,
            allow_missing = True,
            hap = "pat",
            mapped_hap = "pat",
        ),
        smallvar_mat = expand(
            rules.small_var_projected.output,
            allow_missing = True,
            hap = "mat",
            mapped_hap = "mat",
        ),
        annotated=rules.annotate_vcf.output.all,
        hit=rules.get_matches.output.hits,
        v_nohit=rules.get_matches.output.v_nohit,
        g_nohit=rules.get_matches.output.g_nohit,
        gbench=rules.summarize_bench.output.gbench,
    output:
        g_matches = final_dir / "match_summary" / "genome_error_hits.pdf",
        v_matches = final_dir / "match_summary" / "variant_hits.pdf",
        vg_matches = final_dir / "match_summary" / "vg_hits.pdf",
        class_by_type = final_dir / "match_summary" / "class_by_type.pdf",
        class_by_count = final_dir / "match_summary" / "class_by_count.pdf",
        class_by_type_short = final_dir / "match_summary" / "class_by_type_short.pdf",
        class_by_count_short = final_dir / "match_summary" / "class_by_count_short.pdf",
        ave_collapse = final_dir / "match_summary" / "average_collapse.pdf",
        errors_by_region = final_dir / "match_summary" / "errors_by_region.pdf",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/make_plots.R"


rule all:
    input:
        expand(
            rules.plot_match_summary.output,
            ref=["GRCh38"],
            qver=["v1", "v2"],
        ),
        expand(
            rules.get_projection_stats.output,
            ref=["GRCh38"],
            hap=HAPLOTYPES,
            mapped_hap=HAPLOTYPES,
            qver=["v1", "v2"],
        ),
